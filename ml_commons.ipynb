{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200d231b",
   "metadata": {},
   "source": [
    "# Using AWS Opensearch ML Commons REST API for language detection\n",
    "\n",
    "Amazon Opensearch 2.15.0 has a new ML inference processor that enables users to enrich ingest pipelines using inferences from OpenSearch-provided pretrained models. The ml_inference processor is used to invoke machine learning (ML) models registered in the OpenSearch ML Commons plugin. The model outputs are added as new fields to the ingested documents.\n",
    "\n",
    "ML Commons for OpenSearch makes it easy to develop new machine learning features within Opensearch. The plugin allows machine learning engineers and developers to leverage existing opensource machine learning algorithms and streamlines the efforts to build new machine learning features.  \n",
    "\n",
    "We will be deploying two models in this notebook, the first is an Amazon Comprehend model. The model examines the input text, detects the language using the Amazon Comprehend DetectDominantLanguage API, and sets a corresponding language code.\n",
    "\n",
    "The second model is Amazon's Titan embeddings model v2 available on Amazon Bedrock. In this notebook, we will use this embeddings model to create vectors of text in three languages (English, French and German). These vectors will then be stored in Amazon OpenSearch and allow for semantic searches to be used across the language sets.\n",
    "\n",
    "This notebook will walk through creating an Amazon OpenSearch connector, model, ingest pipeline, testing, and vector semantic search.\n",
    "\n",
    "Note: This functionality is available in **Amazon OpenSearch** 2.15.0 or later (we release odd versions), and Opensearch 2.14.0 or later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6104781",
   "metadata": {},
   "source": [
    "#### Step 1. Install dependancies needed for this notebook.\n",
    "\n",
    "Ignore the ERROR about pip's dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd18d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker requests-aws4auth GitPython opensearch-py --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a4a1a",
   "metadata": {},
   "source": [
    "#### Step 2. Install git-lfs so that we can clone the model repos to our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da73a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum install -y amazon-linux-extras\n",
    "!sudo amazon-linux-extras install epel -y \n",
    "!sudo yum-config-manager --enable epel\n",
    "!sudo yum install git-lfs -y\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461ffc63",
   "metadata": {},
   "source": [
    "#### Step 3.  Store the Cloudformation output values as variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0f00c8",
   "metadata": {},
   "source": [
    "This code block will store your cloudformation stack outputs as variables: S3BucketName, SageMakerExecutionRoleArn, SageMakerOpenSearchRoleArn, OpensearchDashboardsURL, OpensearchDomainEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370439a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "cf_client = boto3.client('cloudformation')\n",
    "\n",
    "# Replace 'YourStackName' with the actual name of your CloudFormation stack\n",
    "stack_name = 'opensearch-lang-detect-demo'\n",
    "\n",
    "# Get stack outputs\n",
    "response = cf_client.describe_stacks(StackName=stack_name)\n",
    "outputs = response['Stacks'][0]['Outputs']\n",
    "\n",
    "# Create a dictionary to store the outputs\n",
    "output_dict = {output['OutputKey']: output['OutputValue'] for output in outputs}\n",
    "\n",
    "# Retrieve values from the output dictionary\n",
    "s3BucketName = output_dict.get('S3BucketName', 'Not found')\n",
    "sageMakerExecutionRoleArn = output_dict.get('SageMakerExecutionRoleArn', 'Not found')\n",
    "sageMakerOpenSearchRoleArn = output_dict.get('SageMakerOpenSearchRoleArn', 'Not found')\n",
    "opensearchDomainEndpoint = output_dict.get('OpenSearchDomainEndpoint', 'Not found')\n",
    "\n",
    "print('S3 Bucket Name: ' + s3BucketName)\n",
    "print('SageMaker Execution Role Arn: ' + sageMakerExecutionRoleArn)\n",
    "print('SageMaker OpenSearch Role Arn: ' + sageMakerOpenSearchRoleArn)\n",
    "print('OpenSearch Domain Endpoint: ' + opensearchDomainEndpoint)\n",
    "\n",
    "if 'OpenSearchDashboardsURL' in output_dict:\n",
    "    print('OpenSearch Dashboards URL: ' + output_dict['OpenSearchDashboardsURL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a341e8e5",
   "metadata": {},
   "source": [
    "#### Step 4. Add the SageMaker Execution role to OpenSearch\n",
    "\n",
    "For us to be able to interact with OpenSearch from the notebook we need to allow the SageMaker execution role that was created by the CloudFormationTemplate to perform actions in OpenSearch.\n",
    "\n",
    "Navigate to OpenSearch Dashboard (from the Dashboard URL created in step 3) and login using the username and password you provided when deploying your cloudformation stack.  \n",
    "![Dashboard](images/1_dashboard.png)\n",
    "\n",
    "Then navigate to Security using the left hand menu.\n",
    "![Security](images/2_security.png)\n",
    "\n",
    "Next select **Roles** from the Security left hand menu.\n",
    "![Roles](images/3_roles.png)\n",
    "\n",
    "From the roles screen select **all_access**\n",
    "![all access](images/4_all_access.png)\n",
    "\n",
    "Select the **Mapped users** tab and then click on the **Manage mapping** button.\n",
    "![mapped users](images/5_mapped_users.png)\n",
    "\n",
    "Provide the **SageMaker Execution Role Arn**. (this was printed out in Step 3 above)\n",
    "![mapped users](images/6_backend_roles.png)\n",
    "\n",
    "Click on the **Map** button.\n",
    "\n",
    "Navigate back to the **Roles** screen by using the breadcrumb at the top of the dashboard.\n",
    "\n",
    "Search for the **ml_full_access** role and select it.\n",
    "![mapped users](images/7_ml_full_access.png)\n",
    "\n",
    "Select the **Mapped users** tab and then click on the **Manage mapping** button.\n",
    "![mapped users](images/8_ml_full_access_tabs.png)\n",
    "\n",
    "Provide the **SageMaker OpenSearch Role Arn** (this was printed out in Step 3 above)\n",
    "![mapped users](images/9_add_role.png)\n",
    "\n",
    "Click on the **Map** button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7341b9b3",
   "metadata": {},
   "source": [
    "#### Step 5. Setup the commons connector\n",
    "\n",
    "We need to enable access control for the connector to talk to SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import boto3\n",
    "import time\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "service = 'es'\n",
    "session = boto3.Session()\n",
    "region = session.region_name\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, service, session_token=credentials.token)\n",
    "\n",
    "host= 'https://' + opensearchDomainEndpoint\n",
    "\n",
    "# Register repository\n",
    "path = '/_cluster/settings'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "    \"persistent\": {\n",
    "        \"plugins.ml_commons.trusted_connector_endpoints_regex\": [\n",
    "        \"\"\"^https://runtime\\.sagemaker\\..*[a-z0-9-]\\.amazonaws\\.com/.*$\"\"\",\n",
    "        \"\"\"^https://bedrock-runtime\\..*[a-z0-9-]\\.amazonaws\\.com/.*$\"\"\",\n",
    "        \"\"\"^https://comprehend\\..*[a-z0-9-]\\.amazonaws\\.com$\"\"\"\n",
    "\n",
    "    ]\n",
    "    }\n",
    "    }\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef59b0b",
   "metadata": {},
   "source": [
    "## Comprehend Language Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f85310",
   "metadata": {},
   "source": [
    "The first model will be built from the Amazon Comprehend service. This service examines the input text, detects the language using the Amazon Comprehend DetectDominantLanguage API, and sets a corresponding language code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e6a7f",
   "metadata": {},
   "source": [
    "#### Step 6. Create the connector for the Comprehend Language Classification model\n",
    "\n",
    "Now we will create the connector and model for Amazon Comprehend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend = boto3.client('comprehend', region_name='us-east-1')\n",
    "path = '/_plugins/_ml/connectors/_create'\n",
    "url = host + path\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "payload = {\n",
    "  \"name\": \"Comprehend lang identification\",\n",
    "  \"description\": \"comprehend model\",\n",
    "  \"version\": 1,\n",
    "  \"protocol\": \"aws_sigv4\",\n",
    "  \"credential\": {\n",
    "    \"roleArn\": sageMakerOpenSearchRoleArn\n",
    "  },\n",
    "  \"parameters\": {\n",
    "    \"region\": \"us-east-1\",\n",
    "    \"service_name\": \"comprehend\",\n",
    "    \"api_version\": \"20171127\",\n",
    "    \"api_name\": \"DetectDominantLanguage\",\n",
    "    \"api\": \"Comprehend_${parameters.api_version}.${parameters.api_name}\",\n",
    "    \"response_filter\": \"$\"\n",
    "  },\n",
    "  \"actions\": [\n",
    "    {\n",
    "      \"action_type\": \"predict\",\n",
    "      \"method\": \"POST\",\n",
    "      \"url\": \"https://${parameters.service_name}.${parameters.region}.amazonaws.com\",\n",
    "      \"headers\": {\n",
    "        \"content-type\": \"application/x-amz-json-1.1\",\n",
    "        \"X-Amz-Target\": \"${parameters.api}\"\n",
    "      },\n",
    "      \"request_body\": \"{\\\"Text\\\": \\\"${parameters.Text}\\\"}\" \n",
    "    }\n",
    "  ]\n",
    "}\n",
    "# headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "comprehend_connector_response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "comprehend_connector = comprehend_connector_response.json()[\"connector_id\"]\n",
    "print('Connector id: ' + comprehend_connector)\n",
    "# print(comprehend_connector_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c490574",
   "metadata": {},
   "source": [
    "#### Step 7. Register the Comprehend model\n",
    "\n",
    "We now register the Comprehend model to the model group and the connector that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a191115",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/_register'\n",
    "url = host + path\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "payload = {\n",
    "    \"name\": \"comprehend lang id model\",\n",
    "    \"function_name\": \"remote\",\n",
    "    \"description\": \"test model\",\n",
    "    \"connector_id\": comprehend_connector\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "# print(response.json())\n",
    "comprehend_model_id = response.json()['model_id']\n",
    "print('Model id: ' + comprehend_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a9b132",
   "metadata": {},
   "source": [
    "#### Step 8. Deploy the Comprehend model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ comprehend_model_id + '/_deploy'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b7d6c",
   "metadata": {},
   "source": [
    "#### Step 9. Test the Comprehend model through OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0fe342",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ comprehend_model_id + '/_predict'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "    \"parameters\": {\n",
    "        \"Text\": \"你知道厕所在哪里吗\"\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca70cd",
   "metadata": {},
   "source": [
    "#### Step 10. Create the comprehend index pipeline\n",
    "\n",
    "Now we will create the pipeline for the index, this is how we tell OpenSearch to send the field(s) we wanted translations for to the Comprehend endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c7d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "path = '/_ingest/pipeline/comprehend_language_identification_pipeline'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"description\": \"ingest reviews and identify lang with the comprehend model\",\n",
    "  \"processors\":[\n",
    "    {\n",
    "      \"ml_inference\": {\n",
    "        \"model_id\": comprehend_model_id,\n",
    "        \"input_map\": [\n",
    "            {\n",
    "               \"Text\": \"Text\"\n",
    "            }\n",
    "        ],\n",
    "        \"output_map\": [\n",
    "            {\n",
    "                \n",
    "            \"detected_language\": \"response.Languages[0].LanguageCode\",\n",
    "            \"language_score\": \"response.Languages[0].Score\"\n",
    "            }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f153a9e5",
   "metadata": {},
   "source": [
    "#### Step 11. Create the Comprehend index & test\n",
    "\n",
    "Next we create the index using the pipeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a72f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "index_name = 'comprehend_lang_ident_test01'\n",
    "path = '/' + index_name\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"default_pipeline\": \"comprehend_language_identification_pipeline\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send the PUT request to create the index\n",
    "url = host + path\n",
    "response = requests.put(url, auth=awsauth, json=index_settings, headers=headers)\n",
    "\n",
    "# Print the response\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'comprehend_lang_ident_test01'\n",
    "path = '/' + index_name + '/_doc/'\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Define the document to index\n",
    "document = {\n",
    "    \"Text\": \"parlez vous francais?\"\n",
    "}\n",
    "\n",
    "url = host + path\n",
    "response = requests.post(url, auth=awsauth, json=document, headers=headers)\n",
    "\n",
    "\n",
    "print(response.json())\n",
    "\n",
    "doc_id = response.json()['_id']\n",
    "\n",
    "# Retrieve the indexed document\n",
    "get_path = '/' + index_name + '/_doc/' + doc_id\n",
    "get_url = host + get_path\n",
    "get_response = requests.get(get_url, auth=awsauth, headers=headers)\n",
    "\n",
    "# Print the retrieved document\n",
    "print(get_response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fd32ae",
   "metadata": {},
   "source": [
    "## Amazon Bedrock: Multilingual Vector Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f8761f",
   "metadata": {},
   "source": [
    "In the Sagemaker instance that has been deployed for you, you can see the following files before you click into the notebook you worked on in part 1: english.json, french.json, german.json. These documents have sentences in their respective languages that talk about the term “spring” in different contexts. These contexts include spring as a verb, ie. moving suddenly, includes spring as a noun ie. the season of Spring, and finally spring in the context of a mechanical part. In this section, we will deploy Amazon's Titan embeddings model v2 using the ML commons plugin for Amazon Bedrock. We will then use this embeddings model to create vectors of text in three languages from ingesting the different language json files. Finally, these vectors will be stored in Amazon OpenSearch and allow for semantic searches to be used across the language sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7b460",
   "metadata": {},
   "source": [
    "#### Step 1. Load Sentences from json files into dataframes\n",
    "\n",
    "\n",
    "First we must load the json document sentences into dataframes for more structured organization. Each\n",
    "row can contain the text, embeddings, and additional contextual information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82184aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def load_sentences(file_name):\n",
    "    sentences = []\n",
    "    with open(file_name, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                if 'sentence' in data and 'sentence_english' in data:\n",
    "                    sentences.append({\n",
    "                        'sentence': data['sentence'],\n",
    "                        'sentence_english': data['sentence_english']\n",
    "                    })\n",
    "            except json.JSONDecodeError:\n",
    "                # Skip lines that are not valid JSON (like the index lines)\n",
    "                continue\n",
    "    \n",
    "    return pd.DataFrame(sentences)\n",
    "\n",
    "# Usage\n",
    "german_df = load_sentences('german.json')\n",
    "english_df = load_sentences('english.json')\n",
    "french_df = load_sentences('french.json')\n",
    "# print(french_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65cf369",
   "metadata": {},
   "source": [
    "#### Step 2. Create the OpenSearch Commons Connector to Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a386b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create bedrock connector\n",
    "\n",
    "path = '/_plugins/_ml/connectors/_create'\n",
    "url = host + path\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "payload = {\n",
    "  \"name\": \"Amazon Bedrock Connector: embedding\",\n",
    "  \"description\": \"The connector to bedrock Titan embedding model\",\n",
    "  \"version\": 1,\n",
    "  \"protocol\": \"aws_sigv4\",\n",
    "  \"parameters\": {\n",
    "    \"region\": \"us-east-1\",\n",
    "    \"service_name\": \"bedrock\",\n",
    "    \"model\": \"amazon.titan-embed-text-v2:0\",\n",
    "    \"dimensions\": 1024,\n",
    "    \"normalize\": True,\n",
    "    \"embeddingTypes\": [\"float\"]\n",
    "  },\n",
    "  \"credential\": {\n",
    "    \"roleArn\": sageMakerOpenSearchRoleArn\n",
    "  },\n",
    "  \"actions\": [\n",
    "    {\n",
    "      \"action_type\": \"predict\",\n",
    "      \"method\": \"POST\",\n",
    "      \"url\": \"https://bedrock-runtime.${parameters.region}.amazonaws.com/model/${parameters.model}/invoke\",\n",
    "      \"headers\": {\n",
    "        \"content-type\": \"application/json\",\n",
    "        \"x-amz-content-sha256\": \"required\"\n",
    "      },\n",
    "      \"request_body\": \"{ \\\"inputText\\\": \\\"${parameters.inputText}\\\", \\\"dimensions\\\": ${parameters.dimensions}, \\\"normalize\\\": ${parameters.normalize}, \\\"embeddingTypes\\\": ${parameters.embeddingTypes} }\",\n",
    "      \"pre_process_function\": \"connector.pre_process.bedrock.embedding\",\n",
    "      \"post_process_function\": \"connector.post_process.bedrock.embedding\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "# headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "bedrock_connector_response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "# print(bedrock_connector_response)\n",
    "bedrock_connector_3 = bedrock_connector_response.json()[\"connector_id\"]\n",
    "print('Connector id: ' + bedrock_connector_3)\n",
    "# print(comprehend_connector_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69028489",
   "metadata": {},
   "source": [
    "#### Step 3. Register the Titan embeddings model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b33a9ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/_register'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "    \"name\": \"bedrock multi-modal-embedding\",\n",
    "    \"function_name\": \"remote\",\n",
    "    \"description\": \"test model\",\n",
    "    \"connector_id\": bedrock_connector_3\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "bedrock_model_id = response.json()['model_id']\n",
    "print('Model id: ' + bedrock_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa46a51a",
   "metadata": {},
   "source": [
    "#### Step 4. Deploy the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60893f22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ bedrock_model_id + '/_deploy'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0ac916",
   "metadata": {},
   "source": [
    "#### Step 5. Test the model through Opensearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8616a667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ bedrock_model_id + '/_predict'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"parameters\": {\n",
    "    \"inputText\": \"It's nice to see the flowers bloom and hear the birds sing in the spring\"\n",
    "  }\n",
    "}\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c388f0",
   "metadata": {},
   "source": [
    "#### Step 6. Create Bedrock Titan embeddings model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50267a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import json\n",
    "\n",
    "region = 'us-east-1'\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, service, session_token=credentials.token)\n",
    "\n",
    "pipeline_name = \"titan_embedding_pipeline_v2\"\n",
    "url = f\"{host}/_ingest/pipeline/{pipeline_name}\"\n",
    "\n",
    "pipeline_body = {\n",
    "    \"description\": \"Titan embedding pipeline\",\n",
    "    \"processors\": [\n",
    "        {\n",
    "            \"text_embedding\": {\n",
    "                \"model_id\": bedrock_model_id,\n",
    "                \"field_map\": {\n",
    "                    \"sentence\": \"sentence_vector\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.put(url, auth=awsauth, json=pipeline_body, headers={\"Content-Type\": \"application/json\"})\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c79337a",
   "metadata": {},
   "source": [
    "#### Step 7. Create Bedrock Titan embeddings model index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178e756b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import requests\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "# Set up your AWS credentials and region\n",
    "region = 'us-east-1'\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, service, session_token=credentials.token)\n",
    "\n",
    "# Create the index\n",
    "index_name = 'bedrock-knn-index-v2'\n",
    "url = f'{host}/{index_name}'\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"sentence_vector\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 1024,  \n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"l2\",\n",
    "                    \"engine\": \"nmslib\"\n",
    "                },\n",
    "                \"store\":True\n",
    "            },\n",
    "            \"sentence\":{\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            },\n",
    "            \"sentence_english\":{\n",
    "                \"type\": \"text\",\n",
    "                \"store\": True\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"knn\": True,\n",
    "            \"knn.space_type\": \"cosinesimil\",\n",
    "            \"default_pipeline\": pipeline_name\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.put(url, auth=awsauth, json=mapping, headers={\"Content-Type\": \"application/json\"})\n",
    "print(f\"Index creation response: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8212425c",
   "metadata": {},
   "source": [
    "#### Step 8. Ingest a test doc to ensure that the pipeline is working properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87502515",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_doc = {\n",
    "    \"sentence\": \"Shwetha Test - This is a test sentence for embedding generation.\",\n",
    "    \"sentence_english\": \"Shwetha Test - This is a test sentence for embedding generation.\"\n",
    "}\n",
    "\n",
    "index_url = f\"{host}/{index_name}/_doc\"#?pipeline={pipeline_name}\"\n",
    "index_response = requests.post(index_url, auth=awsauth, json=test_doc, headers={\"Content-Type\": \"application/json\"})\n",
    "print(\"Manual indexing response:\")\n",
    "print(json.dumps(index_response.json(), indent=2))\n",
    "\n",
    "if index_response.status_code == 201:\n",
    "    doc_id = index_response.json()['_id']\n",
    "    # Retrieve the document\n",
    "    get_url = f\"{host}/{index_name}/_doc/{doc_id}\"\n",
    "    get_response = requests.get(get_url, auth=awsauth)\n",
    "    print(\"Retrieved document:\")\n",
    "    print(json.dumps(get_response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dc5547",
   "metadata": {},
   "source": [
    "#### Step 9. Load dataframes into index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b92492f",
   "metadata": {},
   "source": [
    "Index all your documents and generate embeddings for them using the Titan Embeddings Model v2. The embeddings will be stored in the sentence_vector field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1088fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "# Set up your AWS credentials and region\n",
    "region = 'us-east-1'\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "# Define your OpenSearch index name\n",
    "index_name = \"bedrock-knn-index-v2\"\n",
    "\n",
    "def index_documents(df, batch_size=100):\n",
    "    total = len(df)\n",
    "    for start in range(0, total, batch_size):\n",
    "        end = min(start + batch_size, total)\n",
    "        batch = df.iloc[start:end]\n",
    "\n",
    "        bulk_data = []\n",
    "        for _, row in batch.iterrows():\n",
    "            # Prepare the action metadata\n",
    "            action = {\n",
    "                \"index\": {\n",
    "                    \"_index\": index_name\n",
    "                }\n",
    "            }\n",
    "            # Prepare the document data\n",
    "            doc = {\n",
    "                \"sentence\": row['sentence'],\n",
    "                \"sentence_english\": row['sentence_english']\n",
    "            }\n",
    "            \n",
    "            # Add the action and document to the bulk data\n",
    "            bulk_data.append(json.dumps(action))\n",
    "            bulk_data.append(json.dumps(doc))\n",
    "\n",
    "        # Join the bulk data with newlines\n",
    "        bulk_body = \"\\n\".join(bulk_data) + \"\\n\"\n",
    "\n",
    "        # Send the bulk request\n",
    "        bulk_url = f\"{host}/_bulk\"\n",
    "        response = requests.post(bulk_url, auth=awsauth, data=bulk_body, headers={\"Content-Type\": \"application/x-ndjson\"})\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Successfully indexed batch {start}-{end} of {total}\")\n",
    "        else:\n",
    "            print(f\"Error indexing batch {start}-{end} of {total}: {response.text}\")\n",
    "\n",
    "        # Optional: add a small delay to avoid overwhelming the cluster\n",
    "        time.sleep(1)\n",
    "\n",
    "# Index your documents\n",
    "print(\"Indexing German documents:\")\n",
    "index_documents(german_df)\n",
    "print(\"\\nIndexing English documents:\")\n",
    "index_documents(english_df)\n",
    "print(\"\\nIndexing French documents:\")\n",
    "index_documents(french_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c534df6b",
   "metadata": {},
   "source": [
    "#### Step 10. Verify that documents are indexed properly by searching for a document in each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de31866",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_url = f\"{host}/{index_name}/_search\"\n",
    "\n",
    "for df, language in [(german_df, \"German\"), (english_df, \"English\"), (french_df, \"French\")]:\n",
    "    search_query = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"sentence\": df.iloc[0]['sentence']\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"sentence\", \"sentence_english\", \"sentence_vector\"]\n",
    "    }\n",
    "\n",
    "    search_response = requests.get(search_url, auth=awsauth, json=search_query, headers={\"Content-Type\": \"application/json\"})\n",
    "    print(f\"\\nSearch result for {language}:\")\n",
    "    print(json.dumps(search_response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60610fb5",
   "metadata": {},
   "source": [
    "#### Step 11. Perform Semantic KNN search across all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import boto3\n",
    "\n",
    "# Set up your AWS credentials and region\n",
    "region = 'us-east-1'\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, service, session_token=credentials.token)\n",
    "\n",
    "# Define your OpenSearch host and index name\n",
    "\n",
    "index_name = \"bedrock-knn-index-v2\"\n",
    "\n",
    "def semantic_search(query_text, k=5):\n",
    "    search_url = f\"{host}/{index_name}/_search\"\n",
    "    \n",
    "    # First, index the query to generate its embedding\n",
    "    index_doc = {\n",
    "        \"sentence\": query_text,\n",
    "        \"sentence_english\": query_text  # Assuming the query is in English\n",
    "    }\n",
    "    index_url = f\"{host}/{index_name}/_doc\"\n",
    "    index_response = requests.post(index_url, auth=awsauth, json=index_doc, headers={\"Content-Type\": \"application/json\"})\n",
    "    \n",
    "    if index_response.status_code != 201:\n",
    "        print(f\"Failed to index query document: {index_response.text}\")\n",
    "        return []\n",
    "    \n",
    "    # Retrieve the indexed query document to get its vector\n",
    "    doc_id = index_response.json()['_id']\n",
    "    get_url = f\"{host}/{index_name}/_doc/{doc_id}\"\n",
    "    get_response = requests.get(get_url, auth=awsauth)\n",
    "    query_vector = get_response.json()['_source']['sentence_vector']\n",
    "    \n",
    "    # Now perform the KNN search\n",
    "    search_query = {\n",
    "        \"size\": 30,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"sentence_vector\": {\n",
    "                    \"vector\": query_vector,\n",
    "                    \"k\": 30\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\"sentence\", \"sentence_english\"]\n",
    "    }\n",
    "\n",
    "    search_response = requests.post(search_url, auth=awsauth, json=search_query, headers={\"Content-Type\": \"application/json\"})\n",
    "    \n",
    "    if search_response.status_code != 200:\n",
    "        print(f\"Search failed with status code {search_response.status_code}\")\n",
    "        print(search_response.text)\n",
    "        return []\n",
    "\n",
    "    # Clean up - delete the temporary query document\n",
    "    delete_url = f\"{host}/{index_name}/_doc/{doc_id}\"\n",
    "    requests.delete(delete_url, auth=awsauth)\n",
    "\n",
    "    return search_response.json()['hits']['hits']\n",
    "\n",
    "# Example usage\n",
    "query = \"le soleil brille\"\n",
    "results = semantic_search(query)\n",
    "\n",
    "if results:\n",
    "    print(f\"Search results for: '{query}'\")\n",
    "    for result in results:\n",
    "        print(f\"Score: {result['_score']}\")\n",
    "        print(f\"Sentence: {result['_source']['sentence']}\")\n",
    "        print(f\"English: {result['_source']['sentence_english']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No results found or search failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be80b014",
   "metadata": {},
   "source": [
    "#### Step 12. Cleanup\n",
    "Please go to the Cloudformation console and delete the stack you deployed. This will ensure that the resources get terminated after experimenting to avoid incurring unnecessary charges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
